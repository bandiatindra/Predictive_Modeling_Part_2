---
title: "STA 380, Part 2: Exercises 1"
author: "Atindra Bandi"
date: "August 17, 2018"
output: 
  md_document:
  variant: markdown_github
---

```{r,echo = FALSE}
# Setting the seed upfront
set.seed (1234567)
library(ggthemes)
library(corrplot)
library(tidyverse)
library(data.table)

library(ggplot2)
library(ggmap)
library(RColorBrewer)
library(gridExtra)
library(reshape2)
library(plyr)
library(arules)
```


**We are trying to analyze the following key questions from the flights data  -**

**1. What is the frequency of flights which departed/arrived Austin every month**
**2. What is the frequency of flights which departed/arrived Austin in every day of week and hour of day**
**3. What are the most common destinations from Austin**
**4. How does the flight patterns / frequency change for the top destinations over the year**
**5. Among the flights which departed Austin, how many got delayed**
**6. Among the flights which got delayed, what is the average delay time**

```{r,echo = FALSE}
# set the working directory
setwd("C:/Users/bandi/Desktop/Predictive Modeling")

airline = read.csv('./Part 2/STA380/data/ABIA.csv',na.strings=c("","NA"))
names(airline)
attach(airline)
# Converting variables to factor variables
Month = as.factor(Month)
DayofMonth = as.factor(DayofMonth)
DayOfWeek = as.factor(DayOfWeek)
Cancelled = as.factor(Cancelled)
Diverted = as.factor(Diverted)

# Plotting the toal number of flights to and from Austin in every month:
airline = airline[airline$Cancelled ==0,]

toAustin = airline[airline$Dest=='AUS', ]
fromAustin = airline[airline$Origin =='AUS', ]

toAustin_monthly = toAustin %>% 
    group_by(Month) %>%
    tally()

fromAustin_monthly = fromAustin %>% 
    group_by(Month) %>%
    tally()



p1 = ggplot(toAustin_monthly, aes(x=as.factor(Month),y=n,group=1))+
  geom_line() +
  scale_y_continuous(expand = c(0,0),limits =c(0,5000) ) +
  labs(title="# of Flights to Austin by months",fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("# of flights") +
  xlab ("Months")+
  geom_text(aes(label=round(n,1)),vjust=-0.3, size=3,fontface='bold')

p2 = ggplot(fromAustin_monthly, aes(x=as.factor(Month),y=n,group=1))+
  geom_line() +
  scale_y_continuous(expand = c(0,0),limits =c(0,5000) ) +
  labs(title="# of Flights from Austin by months",fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_blank())+
  xlab ("Months")+
  geom_text(aes(label=round(n,1)),vjust=-0.3, size=3,fontface='bold')


# Plot the 2 graphs sided by side
grid.arrange(p1,p2,ncol=2)
```

**As expected the # of flights to and from Austin each month are same. There seems to be a higher number of flights in June while much lower in December(probably because lesser people come here for vacations).**


```{r,echo = FALSE}
toAustin_dayofweek = toAustin %>% 
    group_by(DayOfWeek) %>%
    tally()

fromAustin_dayofweek = fromAustin %>% 
    group_by(DayOfWeek) %>%
    tally()

p1_dayofweek = ggplot(toAustin_dayofweek, aes(x=as.factor(DayOfWeek),y=n,group=1))+
  geom_line() +
  scale_y_continuous(expand = c(0,0),limits =c(0,8000) ) +
  labs(title="# of Flights to Austin by Day",fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("# of flights") +
  xlab ("Day of Week")+
  geom_text(aes(label=round(n,1)),vjust=-0.3, size=3,fontface='bold')

p2_dayofweek = ggplot(fromAustin_dayofweek, aes(x=as.factor(DayOfWeek),y=n,group=1))+
  geom_line() +
  scale_y_continuous(expand = c(0,0),limits =c(0,8000) ) +
  labs(title="# of Flights from Austin by Day",fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_blank())+
  xlab ("Day of Week")+
  geom_text(aes(label=round(n,1)),vjust=-0.3, size=3,fontface='bold')


# Plot the 2 graphs sided by side
grid.arrange(p1_dayofweek,p2_dayofweek,ncol=2)
```


**Now, lets look at the number of flights by each hour of the day**

```{r,echo = FALSE}
# Conveting time to 1-hour windows
fromAustin['hourofday'] = as.integer(fromAustin$DepTime/100)

fromAustin_hourofday = fromAustin %>% 
    group_by(hourofday) %>%
    tally()

toAustin['hourofday'] = as.integer(toAustin$ArrTime/100)

toAustin_hourofday = toAustin %>% 
    group_by(hourofday) %>%
    tally()

p1_hourofday = ggplot(fromAustin_hourofday, aes(x=as.factor(hourofday),y=n))+
  geom_bar(stat = "identity",width = 0.2, fill ="#A45A52") +
  scale_y_continuous(expand = c(0,0),limits =c(0,6000) ) +
  labs(title="Flight frequencies  by hour of day",fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("# of flights") +
  xlab ("Hour of Day")+
  geom_text(aes(label=round(n,1)),vjust=-0.3, size=3,fontface='bold')

p2_hourofday = ggplot(toAustin_hourofday, aes(x=as.factor(hourofday),y=n))+
  geom_bar(stat = "identity",width = 0.2, fill ="#A45A52") +
  scale_y_continuous(expand = c(0,0),limits =c(0,6000) ) +
  labs(title="Flight frequencies  by hour of day",fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("# of flights") +
  xlab ("Hour of Day")+
  geom_text(aes(label=round(n,1)),vjust=-0.3, size=3,fontface='bold')

p1_hourofday
p2_hourofday
```


**Now let's try to understand the delays by hours  - Is it that the hours when there is a higher frequency have higher amounts of delays**

```{r,echo = FALSE}
# let us now look at the number of delays in every hour
fromAustin['departure_delay'] = ifelse(fromAustin$DepDelay >0,"Delay","No Delay")
library(plyr)
fromAustin_delays = fromAustin %>% 
    group_by(Month,departure_delay) %>%
    tally()

# Adding a fraction column
fromAustin_delays$fraction <- with(fromAustin_delays, ave(n,Month, FUN=prop.table))

p1_delays_months = ggplot(fromAustin_delays, aes(x=as.factor(Month),y=fraction,fill=as.factor(departure_delay)))+
  geom_bar(stat = "identity",position = "fill") +
  scale_y_continuous(labels = scales::percent,expand = c(0,0) ) +
  labs(title="% Flights by Delay Status for Each Month",fill = "Departure Delay", fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("% Flights") +
  xlab ("Month") + 
  scale_fill_manual(values = c("#f0650e", "#708238"))+
    geom_text(aes(label=paste0(sprintf("%.0f", fraction*100),"%")),
              position = position_stack(vjust = 0.5), size=4,fontface='bold',
              colour = 'white')

fromAustin_delays_hours = fromAustin %>% 
    group_by(departure_delay,hourofday) %>%
    tally()

fromAustin_delays_hours$fraction <- with(fromAustin_delays_hours, ave(n,hourofday, FUN=prop.table))

p2_delays_hour = ggplot(fromAustin_delays_hours, aes(x=as.factor(hourofday),y=fraction,fill=as.factor(departure_delay) ))+
  geom_bar(stat = "identity",position = "fill") +
  scale_y_continuous(labels = scales::percent,expand = c(0,0) ) +
  labs(title="% Flights by Delay Status for Hour of Day", fill = "Departure Delay",fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("% Flights") +
  xlab ("Hour of Day")+ 
  scale_fill_manual(values = c("#f0650e", "#708238"))+
    geom_text(aes(label=paste0(sprintf("%.0f", fraction*100),"%")),
              position = position_stack(vjust = 0.5), size=3,fontface='bold',
              colour = 'white')


p1_delays_months
p2_delays_hour
```

**Most of the delays happen in the 3rd month (March) and 12th month (December)**
**When looking at hour of day, most of the delays happen after 8 PM till 1 AM. You would notice that there are no flights between 2-4 AM**


**Now among the flights which were delayed, let's caculate the average delay time by month and hour of day**

```{r,echo = FALSE}

fromAustin_delays = fromAustin [fromAustin$departure_delay == 'Delay',]

median_delays = setNames(aggregate(fromAustin_delays$DepDelay,list(fromAustin_delays$Month),median),c("Month","median_delays"))


median_delays_months = ggplot(median_delays, aes(x=as.factor(Month),y=median_delays ))+
  geom_bar(stat = "identity",width = 0.2, fill ="#A45A52") +
  scale_y_continuous(expand = c(0,0),limits =c(0,20) ) +
  labs(title="Median Delay in Minutes by Month",fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("Median Delay (Minutes)") +
  xlab ("Month")+
  geom_text(aes(label=round(median_delays,1)),vjust=-0.3, size=3,fontface='bold')


median_delays = setNames(aggregate(fromAustin_delays$DepDelay,list(fromAustin_delays$hourofday),median),c("hourofday","median_delays"))

median_delays_hour = ggplot(median_delays, aes(x=as.factor(hourofday),y=median_delays ))+
  geom_bar(stat = "identity",width = 0.2, fill ="#A45A52") +
  scale_y_continuous(expand = c(0,0),limits =c(0,400) ) +
  labs(title="Median Delay in Minutes by Hour of Day",fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("Median Delay (Minutes)") +
  xlab ("Hour of Day")+
  geom_text(aes(label=round(median_delays,1)),vjust=-0.3, size=3,fontface='bold')

median_delays_months
median_delays_hour
```


**The median delay is approx. 14 minutes every months. However when we compare across hour of the day we realize that**

```{r,echo = FALSE}

fromAustin_destinations = fromAustin %>% 
    group_by(Dest) %>%
    tally() %>%
  top_n(n = 5, wt = n)

toAustin_destinations = toAustin %>% 
    group_by(Origin) %>%
    tally() %>%
  top_n(n = 5, wt = n)


p1_commondestination = ggplot(fromAustin_destinations, aes(x=reorder(as.factor(Dest),-n),y=n))+
  geom_bar(stat = "identity",width = 0.2, fill ="#A45A52") +
  scale_y_continuous(expand = c(0,0),limits =c(0,8000) ) +
  labs(title="Flight frequencies for top 5 destinations from Austin",fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("# of flights") +
  xlab ("Destination")+
  geom_text(aes(label=round(n,1)),vjust=-0.3, size=4,fontface='bold')
```

Interestingly, most of the flights from Austin are to Dallas and Dallas/Fort Worth

```{r,echo = FALSE}

fromAustin_destinations_monthly = fromAustin %>% 
    group_by(Dest,Month) %>%
    tally()

top5destinations = fromAustin_destinations_monthly[fromAustin_destinations_monthly$Dest %in% as.character(fromAustin_destinations$Dest),]


p1_top5destinations = ggplot(top5destinations, aes(x=as.factor(Month),y=n,group = Dest,colour = Dest))+
  geom_line() +
  scale_color_manual(labels = c("DAL", "DEN","DFW","IAH","PHX"), 
                     values = c( "red","blue","green","yellow","violet"))+
  scale_y_continuous(expand = c(0,0),limits =c(0,650) ) +
  labs(title="Flight frequencies by month for top 5 destinations from Austin",fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("# of flights") +
  xlab ("Months")
```

**Interestingly, the number of flights drop for Dallas in the 7th month!**


**Finally, let's look at the delays by most frequent destinations and the "worst"" destinations.**

**Top destinations - Based on the number of flights going out from Austin**
**Worst Destinations - Based on the proportion of delayed flights**

**DELAYS BY TOP DESTINATIONS**

```{r,echo = FALSE}
# let us now look at the number of delays in every hour
#fromAustin['departure_delay'] = ifelse(fromAustin$DepDelay >0,"Delay","No Delay")

fromAustin_delays = fromAustin [fromAustin$departure_delay == 'Delay',]

top5destinations = fromAustin_delays[fromAustin_delays$Dest %in% as.character(fromAustin_destinations$Dest),]

median_delays = setNames(aggregate(top5destinations$DepDelay,list(top5destinations$Month,top5destinations$Dest),median),c("Month","Dest","median_delays"))

ggplot(median_delays) + geom_tile(aes(x=as.factor(Month), y=as.factor(Dest), fill=median_delays)) + scale_fill_gradient(low = "yellow", high = "red" )  +
  labs(title="Median delays in Minutes for Top 5 Destinations by Months",fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("Destination") +
  xlab ("Months")
    

median_delays = setNames(aggregate(top5destinations$DepDelay,list(top5destinations$hourofday,top5destinations$Dest),median),c("hourofday","Dest","median_delays"))

ggplot(median_delays) + geom_tile(aes(x=as.factor(hourofday), y=as.factor(Dest), fill=median_delays)) + scale_fill_gradient(low = "yellow", high = "red" )  +
  labs(title="Median delays in Minutes for Top 5 Destinations by Hour of Day",fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("Destination") +
  xlab ("Hour of Day")
```

**The first graph shows a heat-map of median delay times for the top destinations against months, while the second graph shows the same against hour of day. Below are some of the insights from these graphs**

* Almost all the destinations have a higher delay time in December
* For DFW and IAH there are huge delays during August. DFW also has high delay times in May
* There does not seem to be a big difference in delay times by the hour of the day except for midnight flights (11 PM-12 AM)

**DELAYS BY "WORST" DESTINATIONS**

```{r,echo = FALSE}
# let us now look at the number of delays in every hour
fromAustin['departure_delay'] = ifelse(fromAustin$DepDelay >0,"Delay","No Delay")

destination_delays = fromAustin %>% 
    group_by(Dest,departure_delay) %>%
    tally()

# Adding a fraction column
destination_delays$fraction <- with(destination_delays, ave(n,Dest, FUN=prop.table))
destination_delays_filter = destination_delays[destination_delays$departure_delay=='Delay',]
# Sorting to get the destinations with the most amounts of delays
worst_destinations = destination_delays_filter[order(-destination_delays_filter$fraction),]

# After seeing the top worst destinations we realize that the top most destination just had one flight, so we can exclude it from our analysis. We are then sunsetting the top 5 after removing the first one and plot them to see the % of flights delayed.
worst_destinations = worst_destinations[2:6,]

ggplot(worst_destinations, aes(x=reorder(as.factor(Dest),-fraction),y=fraction))+
  geom_bar(stat = "identity",fill = 'steelblue',width = 0.4) +
  scale_y_continuous(labels = scales::percent,expand = c(0,0),limits = c(0,0.8)) +
  labs(title="% Flights Delayed for Worst Destinations",fill = "Departure Delay", fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=11,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("% Flights Delayed") +
  xlab ("Destination") + 
  geom_text(aes(label=paste0(sprintf("%.0f", fraction*100),"%")), vjust = -0.3, size=4,fontface='bold',colour = 'black')
```

**OKC, OAK and TPA have one of the highest flight delays and can be classified as one of the "worst" destinations.**

**Overall based on the graphs above we can conclude the following:**

* September, October and November are one of the best months to travel from Austin
* Wihin these months if you take an early morning (5 AM) flights you have the least chances of being late. If however 5 AM is to early delays at 6 AM and 8 AM are not too bad either
* However, if yyou are planning to go to IAH or DFW, maybe you should avoid going in August as there are significant delays in that month
* Lastly be prepared for delays if you are plannig to go to OKC (Oklahoma City), OAK (Oakland) and TPA(Tampa)



## **Practice with association rule mining**

**Below we will try to understand interesting associations between the grocery items boght by customers.**

```{r,echo = FALSE}
library (reshape2)
library (arules)
library (grid)
library (arulesViz)
library(seas)

fc <- file("C:/Users/bandi/Desktop/Predictive Modeling/Part 2/STA380/data/groceries.txt")
mylist <- strsplit(readLines(fc), ",")
close(fc)


# First split data into a list of utilities for each user

# Remove duplicates
groceries = lapply(mylist, unique)

## Cast this variable as a special arules "transactions" class.
groceriestrans = as(groceries, "transactions")

# Now run the 'apriori' algorithm
# Look at rules with support > .001 & confidence >.5 & length (# utilities) <= 5
musicrules = apriori(groceriestrans, 
	parameter=list(support=.001, confidence=.5, maxlen=5))


## There are too many rules  We need the most interesting associations so we will only fiter out the top 10 rules by lift 
inspect(head(musicrules, n = 10, by ="lift"))
```


**Let's also try to visualize these interesting associations in a graph**

``` {r , echo = FALSE}
plot(head(musicrules, n = 10, by = "lift"), method = "graph")

```


**Based on this below are some of the interesting associations -**

* Soda, Pop-corn => Salty Snacks
* Instant food products, soda => Hamburger meat
* Baking powder, flour => Sugar
* Curd => Whole milk
* Curd, other vegetables,whipped/sour-cream,yougurt => cream cheese
* Domestic eggs, procesed cheese => White bread
* Liquor, Red/blush wine => Bottled beer

**Although these are the top associations, we must note that there are only a few transactions for these with very small support.**


## **Author Atribution**

**Below is the code to predict authors for documents in the RC50 (Reuters) dataset.**

```{r,echo = TRUE}
library(tm) 
library(magrittr)
library(randomForest)
library(caret)
library(e1071)

# Setitng the working directory
setwd("C:/Users/bandi/Desktop/Predictive Modeling")

# Function to read the files
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en')
} 
# Get the filenames from the train data set
filenames <- list.files("./Part 2/STA380/data/ReutersC50/C50train", recursive=TRUE)
myname = strsplit(filenames, "[/]")
author_name = NULL
for (i in 1:length(myname)) {
  author_name = c(author_name,myname[[i]][1])
}
class_labels_train = author_name
author_name_train = unique(author_name)

# Get the files in an array
file_list_train <- NULL
for (name in author_name_train){
  file_list_train <- c(file_list_train, Sys.glob(paste0('./Part 2/STA380/data/ReutersC50/C50train/',name,'/*.txt')))
}

# Read all files
all_docs_train = lapply(file_list_train, readerPlain)

mynames_train = file_list_train %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist

# create a dataframe with doc_id as author-article and text as the text in that article
text_vector_train <- NULL

for(i in 1:length(mynames_train)){
  text_vector_train <- c(text_vector_train, paste0(content(all_docs_train[[i]]), collapse = " "))
}

# dataframe with text and document_id
text_df_train <- data.frame(doc_id = mynames_train,
                            text = text_vector_train)

## once you have documents in a vector, you 
## create a text mining 'corpus' with: 
train_raw <- VCorpus(DataframeSource(text_df_train))

## Some pre-processing/tokenization steps.
my_documents_train = train_raw
my_documents_train = tm_map(my_documents_train, content_transformer(tolower))
my_documents_train = tm_map(my_documents_train, content_transformer(removeNumbers))
my_documents_train = tm_map(my_documents_train, content_transformer(removePunctuation))
my_documents_train = tm_map(my_documents_train, content_transformer(stripWhitespace))

# Removing stop words
my_documents_train = tm_map(my_documents_train, content_transformer(removeWords), stopwords("en"))

## create a doc-term-matrix
DTM_train = DocumentTermMatrix(my_documents_train)

# Remove sparse terms
DTM_train = removeSparseTerms(DTM_train, 0.99)
DTM_train # now 3325 terms (versus ~32570 terms before)

# Now, let us repeat the above process of creating DTM for the test data
filenames <- list.files("./Part 2/STA380/data/ReutersC50/C50test", recursive=TRUE)
myname = strsplit(filenames, "[/]")
author_name = NULL
for (i in 1:length(myname)) {
  author_name = c(author_name,myname[[i]][1])
}
class_labels_test = author_name
author_name = unique(author_name)
file_list_test = NULL
#class_labels_test = NULL
for (each in author_name) {
  file_list_test = c(file_list_test,Sys.glob(paste0('./Part 2/STA380/data/ReutersC50/C50test/',each,'/*.txt')))
}
all_docs_test = lapply(file_list_test, readerPlain)
mynames_test = file_list_test %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist

# create a dataframe with doc_id as author-article and text as the text in that article
text_vector_test <- NULL

for(i in 1:length(mynames_test)){
  text_vector_test <- c(text_vector_test, paste0(content(all_docs_test[[i]]), collapse = " "))
}

# dataframe with text and document_id
text_df_test <- data.frame(doc_id = mynames_test,
                            text = text_vector_test)

# convert the dataframe to a Corpus
test_raw <- VCorpus(DataframeSource(text_df_test))

## Some pre-processing/tokenization steps.
## tm_map just maps some function to every document in the corpus
my_documents_test = test_raw
my_documents_test = tm_map(my_documents_test, content_transformer(tolower))
my_documents_test = tm_map(my_documents_test, content_transformer(removeNumbers))
my_documents_test = tm_map(my_documents_test, content_transformer(removePunctuation))
my_documents_test = tm_map(my_documents_test, content_transformer(stripWhitespace))

# Removing stop words
my_documents_test = tm_map(my_documents_test, content_transformer(removeWords), stopwords("en"))

## create a doc-term-matrix
DTM_test = DocumentTermMatrix(my_documents_test)
summary(Terms(DTM_test) %in% Terms(DTM_train))
```

**There are 32,589 words in document-term matrix for the test data, however there are only 3325 words which are also common in the train data set. So, let us drop the remaining words for the classification problem. This is however not an optimal solution, as we are dropping many words.**

```{r,echo = TRUE}
# A suboptimal but practical solution: ignore words you haven't seen before
# can do this by pre-specifying a dictionary in the construction of a DTM
DTM_test2 = DocumentTermMatrix(my_documents_test,
                               control = list(dictionary=Terms(DTM_train)))

# Now checking whether we have all the words from train in test
summary(Terms(DTM_test2) %in% Terms(DTM_train))
```

**Now creating the TF-IDF matrix for test and train data**

```{r,echo = TRUE}
# Now lets create TF-IDF matrix for train and test
tfidf_train = weightTfIdf(DTM_train)
tfidf_test = weightTfIdf(DTM_test2)

# Converting tfidf_train to matrix
tfidf_train = as.matrix(tfidf_train)
tfidf_train = tfidf_train[ , apply(tfidf_train, 2, var) != 0]
```

**3325 words are still high to conduct classification. Thus we will reduce the dimensions using Principal Component Analysis. We will run the PCAs on the train data set and take the top words which explain 75% of the variability in data.**

```{r, echo = TRUE}
# Run PCA on TF-IDF to reduce the number of words
pc_train = prcomp(as.matrix(tfidf_train), scale=TRUE)

# Let's use the top PCs which explain 75% of the variability. So we will take first 330 PCs
X_train = pc_train$x[,1:330]
X_train  = cbind(X_train,class_labels_train)

# Now scale the test data
tfidf_test = as.matrix(tfidf_test)
#tfidf_test = tfidf_test[ , apply(tfidf_test, 2, var) != 0]

#Scaling the test data and applying PCs from train
scaled_tfidf_test = scale(tfidf_test, center=TRUE, scale=TRUE)

X_test <- scaled_tfidf_test %*% pc_train$rotation[,1:330]
#X_test_pc <- as.data.frame(X_test_pc)

#X_test = predict(pc_train, scaled_tfidf_test)
X_test = X_test[,1:330]
```

**Based on the PCA on train data, we can say that 330 words define 75% of the variability. Using the PCAs on the train data set, we predicted the PCAs on test data set. We will use these data sets for our classification of the authors.**

**Classification - 1. Random Forest.**

```{r,echo = TRUE}
X_train = as.data.frame(X_train)

for (name in names(X_train)){
  if (name == "class_labels_train"){
    next
  }else{
    X_train[[name]] <- as.numeric(as.character(X_train[[name]]))
  }
}
X_test = as.data.frame(X_test)
set.seed(99)
author.rf = randomForest(class_labels_train ~., 
                         data = X_train,
                         ntrees = 500,
                         importance = TRUE)

predict_test = predict(author.rf,X_test,type="response")
rf_confusion_matrix = table(predict_test,class_labels_test)
accuracy = sum(diag(rf_confusion_matrix)) / sum(rf_confusion_matrix)
accuracy
```

**After running Random Forest we have an acuracy of 58%. Let us however look at how does the accuracy varyfor different authors**

```{r,echo = TRUE}
library(dplyr)
accurate_authors = as.data.frame(cbind(unique(class_labels_train),
                                       diag(rf_confusion_matrix)))
colnames(accurate_authors) = c("Authors","Correct_Predictions")
accurate_authors$Correct_Predictions =  as.numeric(as.character(accurate_authors$Correct_Predictions))
accurate_authors$Authors = as.character(accurate_authors$Authors)
accurate_authors_sorted = accurate_authors[order(-accurate_authors$Correct_Predictions),]
accurate_authors_sorted$percentage_accuracy = accurate_authors_sorted$Correct_Predictions/50
head(accurate_authors_sorted,10)

# Plotting the top accurately predicted authors
top_n(accurate_authors_sorted, n=5, percentage_accuracy) %>%
          ggplot(., aes(x=reorder(Authors,-percentage_accuracy), y=percentage_accuracy))+
  geom_bar(stat = "identity",fill = 'steelblue',width = 0.4) +
  scale_y_continuous(labels = scales::percent,expand = c(0,0),limits = c(0,1)) +
  labs(title="% Documents Correctly Predicted for Top Predicted Authors",fill = "Departure Delay", fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=9,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("% Accuracy") +
  xlab ("Authors") + 
  geom_text(aes(label=paste0(sprintf("%.0f", percentage_accuracy*100),"%")), vjust = 1, size=4,fontface='bold',colour = 'white')

#PLot the authors which are not predicted that well - 

top_n(accurate_authors_sorted, n=-5, percentage_accuracy) %>%
          ggplot(., aes(x=reorder(Authors,-percentage_accuracy), y=percentage_accuracy))+
  geom_bar(stat = "identity",fill = 'steelblue',width = 0.4) +
  scale_y_continuous(labels = scales::percent,expand = c(0,0),limits = c(0,1)) +
  labs(title="% Documents Correctly Predicted for Bottom Predicted Authors",fill = "Departure Delay", fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=9,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("% Accuracy") +
  xlab ("Authors") + 
  geom_text(aes(label=paste0(sprintf("%.0f", percentage_accuracy*100),"%")), vjust = 1, size=4,fontface='bold',colour = 'white')
```

**The authors most correctly predicted by Random forest are - JimGilchrist, LynnleyBrowning, KarlPenhaul, RobinSidel, MatthewBunce, NickLouth.**

**The authors most incorrectly predicted are - TanEeLyn, ScottHillis, EdnaFernandes, BenjaminKangLim, DarrenSchuettler.**

**2. Support Vector Machine (SVM)**

```{r,echo = TRUE}
set.seed(99)
X_train_svm = subset(X_train, select = -class_labels_train) 
y_train_svm = as.factor(class_labels_train)
model_svm = svm(X_train_svm, y_train_svm, probability = TRUE)
pred_prob = predict(model_svm, X_test, decision.values = TRUE, probability = TRUE)

cm_svm = table(pred_prob,class_labels_test)
accuracy_svm = sum(diag(cm_svm)) / sum(cm_svm)
accuracy_svm
```

**From SVM as well we get an accuracy of 57%.**

**Interestingly, we get similar accuraies in the SVM model as well. Additionally, the run time for SVM is much lower than Random Forest algorithm.**

```{r,echo = TRUE}
library(dplyr)
accurate_authors = as.data.frame(cbind(unique(class_labels_train),
                                       diag(cm_svm)))
colnames(accurate_authors) = c("Authors","Correct_Predictions")
accurate_authors$Correct_Predictions =  as.numeric(as.character(accurate_authors$Correct_Predictions))
accurate_authors$Authors = as.character(accurate_authors$Authors)
accurate_authors_sorted = accurate_authors[order(-accurate_authors$Correct_Predictions),]
accurate_authors_sorted$percentage_accuracy = accurate_authors_sorted$Correct_Predictions/50
head(accurate_authors_sorted,10)

# Plotting the top accurately predicted authors
top_n(accurate_authors_sorted, n=5, percentage_accuracy) %>%
          ggplot(., aes(x=reorder(Authors,-percentage_accuracy), y=percentage_accuracy))+
  geom_bar(stat = "identity",fill = 'steelblue',width = 0.4) +
  scale_y_continuous(labels = scales::percent,expand = c(0,0),limits = c(0,1)) +
  labs(title="% Documents Correctly Predicted for Top Predicted Authors",fill = "Departure Delay", fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=8,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=10,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("% Accuracy") +
  xlab ("Authors") + 
  geom_text(aes(label=paste0(sprintf("%.0f", percentage_accuracy*100),"%")), vjust = 1, size=4,fontface='bold',colour = 'white')

#PLot the authors which are not predicted that well - 

top_n(accurate_authors_sorted, n=-5, percentage_accuracy) %>%
          ggplot(., aes(x=reorder(Authors,-percentage_accuracy), y=percentage_accuracy))+
  geom_bar(stat = "identity",fill = 'steelblue',width = 0.4) +
  scale_y_continuous(labels = scales::percent,expand = c(0,0),limits = c(0,1)) +
  labs(title="% Documents Correctly Predicted for Bottom Predicted Authors",fill = "Departure Delay", fontface='bold')+
  theme_classic()+
  theme(axis.text.x = element_text(colour="grey20",size=8,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=10,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("% Accuracy") +
  xlab ("Authors") + 
  geom_text(aes(label=paste0(sprintf("%.0f", percentage_accuracy*100),"%")), vjust = 1, size=4,fontface='bold',colour = 'white')
```

**The authors most correctly predicted by SVM are - LynnleyBrowning, JimGilchrist, GrahamEarnshaw, BradDorfman, MatthewBunce, NickLouth, TheresePoletti.**

**The authors most incorrectly predicted are - ScottHillis, DarrenSchuettler, DavidLawder, JanLopatka, BenjaminKangLim.**

**Overall, the outputs of the 2 model does give a similar accuracy of ~58%. While this is not impressive, we do get a lot of authors which have very hgh accuracies in both the models. Overall accuracy seems to be low as there are some authors who are not predicted that well. One potential reason behind this could be that we have dropped quite a few words form the train and test data sets. However, there are many more words in the test data (which if incorporated could improve accuracies).**
